"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ’¬ CHAT SERVICE v4.0 â€” ENRICHISSEMENT PERPLEXITY PROGRESSIF                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  NOUVEAUTÃ‰S v4.0:                                                                  â•‘
â•‘  â€¢ ğŸŒ Enrichissement Perplexity automatique selon le plan                          â•‘
â•‘  â€¢ ğŸ” VÃ©rification des faits en temps rÃ©el                                         â•‘
â•‘  â€¢ ğŸ“Š Sources web intÃ©grÃ©es dans les rÃ©ponses                                      â•‘
â•‘  â€¢ ğŸ¯ Fusion intelligente Mistral + Perplexity                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import httpx
from datetime import datetime, date
from typing import Optional, List, Dict, Any, Tuple, AsyncGenerator
from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from db.database import ChatMessage, ChatQuota, Summary, User, WebSearchUsage
from core.config import get_mistral_key, get_perplexity_key, get_openai_key, is_openai_available, PLAN_LIMITS


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ SEMANTIC SCHOLAR API (Sources acadÃ©miques gratuites)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SEMANTIC_SCHOLAR_API = "https://api.semanticscholar.org/graph/v1"

async def search_semantic_scholar(
    query: str,
    limit: int = 5,
    fields: str = "title,authors,year,abstract,url,citationCount"
) -> List[Dict[str, Any]]:
    """
    Recherche des articles acadÃ©miques sur Semantic Scholar (API gratuite).

    Args:
        query: Terme de recherche
        limit: Nombre de rÃ©sultats (max 100)
        fields: Champs Ã  retourner

    Returns:
        Liste de papers avec titre, auteurs, annÃ©e, abstract, url
    """
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{SEMANTIC_SCHOLAR_API}/paper/search",
                params={
                    "query": query,
                    "limit": limit,
                    "fields": fields
                },
                timeout=10
            )

            if response.status_code == 200:
                data = response.json()
                papers = data.get("data", [])

                # Formater les rÃ©sultats
                results = []
                for paper in papers:
                    if paper.get("title"):
                        authors = paper.get("authors", [])
                        author_names = ", ".join([a.get("name", "") for a in authors[:3]])
                        if len(authors) > 3:
                            author_names += " et al."

                        results.append({
                            "title": paper.get("title"),
                            "authors": author_names,
                            "year": paper.get("year"),
                            "abstract": paper.get("abstract", "")[:500] if paper.get("abstract") else None,
                            "url": paper.get("url"),
                            "citations": paper.get("citationCount", 0)
                        })

                print(f"ğŸ“š [SCHOLAR] Found {len(results)} papers for '{query}'", flush=True)
                return results
            else:
                print(f"âš ï¸ [SCHOLAR] API error: {response.status_code}", flush=True)
                return []

    except Exception as e:
        print(f"âŒ [SCHOLAR] Error: {e}", flush=True)
        return []


def _detect_complex_question(question: str) -> bool:
    """
    DÃ©tecte si une question est complexe et nÃ©cessite GPT-4.

    Questions complexes:
    - Analyses comparatives
    - Questions multi-Ã©tapes
    - Raisonnement abstrait
    - SynthÃ¨se de concepts
    """
    question_lower = question.lower()

    COMPLEX_PATTERNS = [
        # Analyses comparatives
        "compare", "diffÃ©rence entre", "avantages et inconvÃ©nients",
        "difference between", "pros and cons", "versus", " vs ",
        # Multi-Ã©tapes
        "Ã©tapes pour", "comment faire pour", "processus de",
        "steps to", "how to", "process of",
        # Raisonnement abstrait
        "pourquoi", "implications", "consÃ©quences",
        "why", "implications", "consequences",
        # SynthÃ¨se
        "rÃ©sume et analyse", "synthÃ©tise", "en quoi",
        "summarize and analyze", "synthesize",
        # Questions longues (>20 mots)
    ]

    # Question longue = probablement complexe
    if len(question.split()) > 20:
        return True

    for pattern in COMPLEX_PATTERNS:
        if pattern in question_lower:
            return True

    return False


async def select_chat_model(question: str, user_plan: str) -> str:
    """
    SÃ©lectionne le modÃ¨le appropriÃ© selon la question et le plan.

    - Free/Starter: Toujours Mistral Small
    - Pro/Expert: GPT-4 pour questions complexes, Mistral sinon
    """
    # Plans basiques: toujours Mistral
    if user_plan in ["free", "student", "starter"]:
        return "mistral"

    # Plans premium: GPT-4 pour questions complexes
    if user_plan in ["pro", "expert", "team", "unlimited"]:
        if _detect_complex_question(question) and is_openai_available():
            return "openai"

    return "mistral"


async def generate_openai_response(
    question: str,
    system_prompt: str,
    user_prompt: str,
    max_tokens: int = 4000
) -> Optional[str]:
    """
    GÃ©nÃ¨re une rÃ©ponse avec OpenAI GPT-4 pour les questions complexes.
    """
    api_key = get_openai_key()
    if not api_key:
        return None

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-4-turbo-preview",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "max_tokens": max_tokens,
                    "temperature": 0.3
                },
                timeout=90
            )

            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"].strip()
            else:
                print(f"âŒ [OpenAI] API error: {response.status_code}", flush=True)
                return None

    except Exception as e:
        print(f"âŒ [OpenAI] Error: {e}", flush=True)
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ IMPORT DU SERVICE D'ENRICHISSEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    from videos.web_enrichment import (
        enrich_chat_response, get_enrichment_level, get_enrichment_badge,
        EnrichmentLevel, get_enrichment_config
    )
    ENRICHMENT_AVAILABLE = True
except ImportError:
    ENRICHMENT_AVAILABLE = False
    print("âš ï¸ [CHAT] Web enrichment not available", flush=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š QUOTAS CHAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def check_chat_quota(
    session: AsyncSession,
    user_id: int,
    summary_id: int
) -> Tuple[bool, str, Dict[str, int]]:
    """
    VÃ©rifie si l'utilisateur peut poser une question.
    Retourne: (can_ask, reason, quota_info)
    """
    result = await session.execute(select(User).where(User.id == user_id))
    user = result.scalar_one_or_none()
    
    if not user:
        return False, "user_not_found", {}
    
    plan = user.plan or "free"
    limits = PLAN_LIMITS.get(plan, PLAN_LIMITS["free"])
    
    daily_limit = limits.get("chat_daily_limit", 10)
    per_video_limit = limits.get("chat_per_video_limit", 5)
    
    # -1 = illimitÃ©
    if daily_limit == -1 and per_video_limit == -1:
        return True, "unlimited", {"daily_limit": -1, "per_video_limit": -1}
    
    today = date.today().isoformat()
    
    # VÃ©rifier le quota journalier
    if daily_limit != -1:
        daily_result = await session.execute(
            select(ChatQuota).where(
                ChatQuota.user_id == user_id,
                ChatQuota.quota_date == today
            )
        )
        daily_quota = daily_result.scalar_one_or_none()
        daily_used = daily_quota.daily_count if daily_quota else 0
        
        if daily_used >= daily_limit:
            return False, "daily_limit_reached", {
                "daily_limit": daily_limit,
                "daily_used": daily_used,
                "per_video_limit": per_video_limit
            }
    else:
        daily_used = 0
    
    # VÃ©rifier le quota par vidÃ©o
    if per_video_limit != -1:
        video_result = await session.execute(
            select(func.count(ChatMessage.id)).where(
                ChatMessage.user_id == user_id,
                ChatMessage.summary_id == summary_id,
                ChatMessage.role == "user"
            )
        )
        video_used = video_result.scalar() or 0
        
        if video_used >= per_video_limit:
            return False, "video_limit_reached", {
                "daily_limit": daily_limit,
                "daily_used": daily_used,
                "per_video_limit": per_video_limit,
                "video_used": video_used
            }
    else:
        video_used = 0
    
    return True, "ok", {
        "daily_limit": daily_limit,
        "daily_used": daily_used,
        "per_video_limit": per_video_limit,
        "video_used": video_used
    }


async def increment_chat_quota(session: AsyncSession, user_id: int):
    """IncrÃ©mente le quota de chat journalier"""
    today = date.today().isoformat()
    
    result = await session.execute(
        select(ChatQuota).where(
            ChatQuota.user_id == user_id,
            ChatQuota.quota_date == today
        )
    )
    quota = result.scalar_one_or_none()
    
    if quota:
        quota.daily_count += 1
    else:
        quota = ChatQuota(
            user_id=user_id,
            quota_date=today,
            daily_count=1
        )
        session.add(quota)
    
    await session.commit()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¬ MESSAGES CHAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def save_chat_message(
    session: AsyncSession,
    user_id: int,
    summary_id: int,
    role: str,
    content: str,
    web_search_used: bool = False,
    fact_checked: bool = False,
    sources: list = None,
    enrichment_level: str = None
) -> int:
    """
    Sauvegarde un message de chat avec mÃ©tadonnÃ©es v5.0.
    âš ï¸ RÃ©trocompatible avec les anciennes bases sans nouvelles colonnes
    
    Args:
        session: Session DB
        user_id: ID utilisateur
        summary_id: ID du rÃ©sumÃ©
        role: 'user' ou 'assistant'
        content: Contenu du message
        web_search_used: Si la recherche web a Ã©tÃ© utilisÃ©e
        fact_checked: Si le message a Ã©tÃ© fact-checkÃ©
        sources: Liste des sources web [{title, url, ...}]
        enrichment_level: Niveau d'enrichissement (none, light, full, deep)
    """
    import json
    from sqlalchemy import text
    
    try:
        # ğŸ†• Essayer d'abord avec toutes les colonnes (v5.0+)
        message = ChatMessage(
            user_id=user_id,
            summary_id=summary_id,
            role=role,
            content=content,
            web_search_used=web_search_used,
            fact_checked=fact_checked,
            sources_json=json.dumps(sources) if sources else None,
            enrichment_level=enrichment_level
        )
        session.add(message)
        await session.commit()
        await session.refresh(message)
        return message.id
        
    except Exception as e:
        # âš ï¸ Fallback: Les nouvelles colonnes n'existent pas encore
        print(f"âš ï¸ [save_chat_message] Fallback to basic columns: {e}", flush=True)
        await session.rollback()
        
        try:
            # Insertion SQL brute avec uniquement les colonnes de base
            result = await session.execute(
                text("""
                    INSERT INTO chat_messages (user_id, summary_id, role, content, created_at)
                    VALUES (:user_id, :summary_id, :role, :content, NOW())
                    RETURNING id
                """),
                {
                    "user_id": user_id,
                    "summary_id": summary_id,
                    "role": role,
                    "content": content
                }
            )
            await session.commit()
            row = result.fetchone()
            return row[0] if row else 0
        except Exception as e2:
            print(f"âŒ [save_chat_message] Error: {e2}", flush=True)
            await session.rollback()
            return 0


async def get_chat_history(
    session: AsyncSession,
    summary_id: int,
    user_id: int,
    limit: int = 50  # AugmentÃ© pour plus d'historique
) -> List[Dict[str, Any]]:
    """
    RÃ©cupÃ¨re l'historique de chat pour une vidÃ©o.
    ğŸ†• v5.0: Inclut les mÃ©tadonnÃ©es (sources, fact-check)
    âš ï¸ RÃ©trocompatible avec les anciennes bases sans nouvelles colonnes
    """
    import json
    from sqlalchemy import text
    
    try:
        # ğŸ†• Essayer d'abord avec les nouvelles colonnes (v5.0+)
        result = await session.execute(
            select(ChatMessage)
            .where(
                ChatMessage.summary_id == summary_id,
                ChatMessage.user_id == user_id
            )
            .order_by(ChatMessage.created_at.desc())
            .limit(limit)
        )
        messages = result.scalars().all()
        
        history = []
        for m in reversed(messages):
            msg_data = {
                "id": m.id,
                "role": m.role,
                "content": m.content,
                "created_at": m.created_at.isoformat() if m.created_at else None
            }
            
            # Ajouter les mÃ©tadonnÃ©es v5.0 si disponibles
            if hasattr(m, 'web_search_used') and m.web_search_used:
                msg_data["web_search_used"] = True
            if hasattr(m, 'fact_checked') and m.fact_checked:
                msg_data["fact_checked"] = True
            if hasattr(m, 'sources_json') and m.sources_json:
                try:
                    msg_data["sources"] = json.loads(m.sources_json)
                except:
                    pass
            if hasattr(m, 'enrichment_level') and m.enrichment_level:
                msg_data["enrichment_level"] = m.enrichment_level
            
            history.append(msg_data)
        
        return history
        
    except Exception as e:
        # âš ï¸ Fallback: Les nouvelles colonnes n'existent pas encore
        # Utiliser une requÃªte SQL brute avec uniquement les colonnes de base
        print(f"âš ï¸ [chat_history] Fallback to basic columns: {e}", flush=True)
        
        # âš ï¸ IMPORTANT: Rollback la transaction Ã©chouÃ©e avant de continuer
        await session.rollback()
        
        try:
            result = await session.execute(
                text("""
                    SELECT id, role, content, created_at 
                    FROM chat_messages 
                    WHERE summary_id = :summary_id AND user_id = :user_id
                    ORDER BY created_at DESC
                    LIMIT :limit
                """),
                {"summary_id": summary_id, "user_id": user_id, "limit": limit}
            )
            rows = result.fetchall()
            
            history = []
            for row in reversed(rows):
                msg_data = {
                    "id": row[0],
                    "role": row[1],
                    "content": row[2],
                    "created_at": row[3].isoformat() if row[3] else None
                }
                history.append(msg_data)
            
            return history
        except Exception as e2:
            print(f"âŒ [chat_history] Error: {e2}", flush=True)
            await session.rollback()
            return []


async def clear_chat_history(
    session: AsyncSession,
    summary_id: int,
    user_id: int
) -> int:
    """Efface l'historique de chat pour une vidÃ©o"""
    from sqlalchemy import delete
    
    result = await session.execute(
        delete(ChatMessage).where(
            ChatMessage.summary_id == summary_id,
            ChatMessage.user_id == user_id
        )
    )
    await session.commit()
    return result.rowcount


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– GÃ‰NÃ‰RATION RÃ‰PONSE CHAT v4.0
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_chat_prompt(
    question: str,
    video_title: str,
    transcript: str,
    summary: str,
    chat_history: List[Dict],
    mode: str,
    lang: str
) -> Tuple[str, str]:
    """
    Construit le prompt pour le chat.
    Retourne: (system_prompt, user_prompt)
    """
    MODE_CONFIG = {
        "accessible": {
            "max_context": 8000,
            "style_fr": "RÃ©ponds de faÃ§on concise (2-4 phrases). Langage simple, accessible.",
            "style_en": "Answer concisely (2-4 sentences). Simple, accessible language."
        },
        "standard": {
            "max_context": 15000,
            "style_fr": "RÃ©ponds de faÃ§on complÃ¨te (4-8 phrases). Ã‰quilibre clartÃ© et dÃ©tail.",
            "style_en": "Answer completely (4-8 sentences). Balance clarity and detail."
        },
        "expert": {
            "max_context": 25000,
            "style_fr": "RÃ©ponds de faÃ§on exhaustive et rigoureuse. Analyse critique.",
            "style_en": "Answer exhaustively and rigorously. Critical analysis."
        }
    }
    
    config = MODE_CONFIG.get(mode, MODE_CONFIG["standard"])
    style = config["style_fr"] if lang == "fr" else config["style_en"]
    max_context = config["max_context"]
    
    # Construire l'historique
    history_text = ""
    if chat_history:
        for msg in chat_history[-6:]:
            role = "Utilisateur" if msg["role"] == "user" else "Assistant"
            history_text += f"\n{role}: {msg['content']}"
    
    # Tronquer le transcript si nÃ©cessaire
    transcript_truncated = transcript[:max_context] if transcript else ""
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ§  DÃ‰TECTION INTELLIGENTE DU TYPE DE QUESTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    question_lower = question.lower().strip()
    
    # DÃ©terminer le type de rÃ©ponse attendue
    FACTUAL_PATTERNS = [
        "c'est quoi", "qu'est-ce que", "qui est", "combien", "quand", "oÃ¹",
        "what is", "who is", "how many", "when", "where", "define",
        "quelle est", "quel est", "donne-moi", "cite", "liste", "Ã©numÃ¨re"
    ]
    
    SUMMARY_PATTERNS = [
        "rÃ©sume", "rÃ©sumÃ©", "synthÃ¨se", "en bref", "principaux points",
        "summarize", "summary", "main points", "key takeaways", "tldr",
        "bullet points", "grandes lignes", "idÃ©es principales"
    ]
    
    YES_NO_PATTERNS = [
        "est-ce que", "est-il", "peut-on", "y a-t-il", "faut-il",
        "is it", "does it", "can we", "should", "is there", "are there"
    ]
    
    DEEP_ANALYSIS_PATTERNS = [
        "explique", "pourquoi", "comment", "analyse", "compare", "diffÃ©rence",
        "explain", "why", "how does", "analyze", "compare", "difference",
        "avantages", "inconvÃ©nients", "implications", "consÃ©quences"
    ]
    
    # Classifier la question
    is_factual = any(p in question_lower for p in FACTUAL_PATTERNS)
    is_summary = any(p in question_lower for p in SUMMARY_PATTERNS)
    is_yes_no = any(p in question_lower for p in YES_NO_PATTERNS)
    is_deep = any(p in question_lower for p in DEEP_ANALYSIS_PATTERNS)
    
    # DÃ©terminer le format de rÃ©ponse idÃ©al
    if is_yes_no:
        response_guide_fr = """ğŸ“Œ FORMAT: Commence par OUI/NON direct, puis explique en 1-2 phrases max.
Exemple: "Oui, la vidÃ©o confirme cela Ã  (3:45). L'auteur prÃ©cise que..." """
        response_guide_en = """ğŸ“Œ FORMAT: Start with YES/NO, then explain in 1-2 sentences max.
Example: "Yes, the video confirms this at (3:45). The author states that..." """
    elif is_factual:
        response_guide_fr = """ğŸ“Œ FORMAT: RÃ©ponse factuelle DIRECTE en 1-3 phrases. Pas de prÃ©ambule.
âœ… "Le concept X est dÃ©fini Ã  (2:30) comme..."
âŒ "C'est une excellente question ! Dans la vidÃ©o..." """
        response_guide_en = """ğŸ“Œ FORMAT: DIRECT factual answer in 1-3 sentences. No preamble.
âœ… "Concept X is defined at (2:30) as..."
âŒ "That's a great question! In the video..." """
    elif is_summary:
        response_guide_fr = """ğŸ“Œ FORMAT: Liste Ã  puces concise (3-5 points max). Pas de longs paragraphes.
Structure:
â€¢ Point 1 (timecode)
â€¢ Point 2 (timecode)
â€¢ Point 3 (timecode)"""
        response_guide_en = """ğŸ“Œ FORMAT: Concise bullet list (3-5 points max). No long paragraphs.
Structure:
â€¢ Point 1 (timecode)
â€¢ Point 2 (timecode)
â€¢ Point 3 (timecode)"""
    elif is_deep:
        response_guide_fr = """ğŸ“Œ FORMAT: Analyse structurÃ©e mais ciblÃ©e. RÃ©ponds Ã  la question, pas plus.
â€¢ Commence par la rÃ©ponse directe
â€¢ DÃ©veloppe avec preuves de la vidÃ©o
â€¢ Conclusion en 1 phrase si nÃ©cessaire"""
        response_guide_en = """ğŸ“Œ FORMAT: Structured but focused analysis. Answer the question, nothing more.
â€¢ Start with direct answer
â€¢ Develop with video evidence
â€¢ 1 sentence conclusion if needed"""
    else:
        response_guide_fr = """ğŸ“Œ FORMAT: Adapte la longueur Ã  la complexitÃ©. 
â€¢ Question simple â†’ rÃ©ponse courte (2-3 phrases)
â€¢ Question complexe â†’ rÃ©ponse dÃ©veloppÃ©e"""
        response_guide_en = """ğŸ“Œ FORMAT: Adapt length to complexity.
â€¢ Simple question â†’ short answer (2-3 sentences)  
â€¢ Complex question â†’ developed answer"""
    
    response_guide = response_guide_fr if lang == "fr" else response_guide_en
    
    if lang == "fr":
        system_prompt = f"""Tu es Deep Sight AI, un assistant expert pour analyser les vidÃ©os YouTube.

ğŸ“º VIDÃ‰O ANALYSÃ‰E: {video_title}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ MISSION: RÃ©pondre de maniÃ¨re EXPERTE et PÃ‰DAGOGIQUE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ LANGUE: Tu dois rÃ©pondre UNIQUEMENT en franÃ§ais parfait.
â€¢ Utilise un franÃ§ais acadÃ©mique, clair et bien structurÃ©
â€¢ Ã‰vite les anglicismes inutiles
â€¢ Formule des phrases Ã©lÃ©gantes et professionnelles

{response_guide}

ğŸ“ FORMAT DE RÃ‰PONSE PRO:
â€¢ Structure claire avec sections si nÃ©cessaire
â€¢ Utilise **gras** pour les concepts clÃ©s
â€¢ Utilise > pour les citations importantes
â€¢ Timecodes au format (MM:SS) pour chaque affirmation

ğŸ“š CONCEPTS INTERACTIFS (IMPORTANT):
Entoure les termes techniques, noms propres et concepts importants avec [[double crochets]].
Exemples:
â€¢ "La [[photosynthÃ¨se]] permet aux plantes de..."
â€¢ "Selon [[Albert Einstein]], la [[relativitÃ© gÃ©nÃ©rale]] explique..."
â€¢ "Le [[PIB]] a augmentÃ© de 3%..."
â€¢ "Cette technique utilise l'[[apprentissage profond]]..."
Marque 3-6 concepts par rÃ©ponse, PAS PLUS. Choisis les plus importants/Ã©ducatifs.

â±ï¸ CITATIONS AVEC TIMECODES:
âœ… "Selon l'auteur Ã  **(5:23)**, le concept X signifie..."
âœ… "Ã€ **(12:45)**, il est expliquÃ© que..."

âš ï¸ IMPORTANT - DISTINCTION VIDÃ‰O vs RÃ‰ALITÃ‰:
â€¢ Si la question porte sur des FAITS VÃ‰RIFIABLES (dates, Ã©vÃ©nements):
  â†’ PrÃ©cise "**Dans cette vidÃ©o**, il est dit que..." 
â€¢ Si c'est une vidÃ©o HUMORISTIQUE/PARODIQUE:
  â†’ Mentionne-le: "Cette vidÃ©o est une **parodie**..."

ğŸš« INTERDICTIONS:
â€¢ Pas de prÃ©ambules ("Excellente question", "Bien sÃ»r")
â€¢ Pas de conclusions gÃ©nÃ©riques
â€¢ Ne pas halluciner d'informations non prÃ©sentes
â€¢ Ne JAMAIS mÃ©langer anglais et franÃ§ais

âœ… COMPORTEMENT ATTENDU:
â€¢ RÃ©ponse DIRECTE dÃ¨s la premiÃ¨re phrase
â€¢ Structuration claire et professionnelle
â€¢ HonnÃªtetÃ© si l'info n'est pas dans la vidÃ©o

ğŸ’¡ PISTES DE RÃ‰FLEXION (OBLIGATOIRE):
Ã€ la fin de CHAQUE rÃ©ponse, ajoute une section avec EXACTEMENT ce format:

---
**ğŸ”® Pour aller plus loin :**
[ask:Question pertinente 1 basÃ©e sur la vidÃ©o]
[ask:Question pertinente 2 pour approfondir]
[ask:Question pertinente 3 pour Ã©largir]

âš ï¸ FORMAT CRITIQUE: Chaque question DOIT Ãªtre sur sa propre ligne, commencer par [ask: et finir par ]
Ces questions doivent Ãªtre concrÃ¨tes, spÃ©cifiques au contenu de la vidÃ©o, et inciter l'utilisateur Ã  explorer davantage.
"""
        
        user_prompt = f"""ğŸ“‹ RÃ‰SUMÃ‰ DE LA VIDÃ‰O:
{summary[:4000] if summary else "Non disponible"}

ğŸ“ TRANSCRIPTION COMPLÃˆTE:
{transcript_truncated}

ğŸ’¬ HISTORIQUE DU CHAT:{history_text}

â“ QUESTION DE L'UTILISATEUR: {question}

ğŸ“ RÃ©ponds de maniÃ¨re EXPERTE et STRUCTURÃ‰E:"""

    else:
        system_prompt = f"""You are Deep Sight AI, an expert assistant for analyzing YouTube videos.

ğŸ“º ANALYZED VIDEO: {video_title}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ MISSION: Respond in an EXPERT and PEDAGOGICAL manner
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ LANGUAGE: You MUST respond ONLY in perfect English.
â€¢ Use clear, academic, well-structured English
â€¢ Formulate elegant and professional sentences
â€¢ Avoid colloquialisms unless quoting the video

{response_guide}

ğŸ“ PRO RESPONSE FORMAT:
â€¢ Clear structure with sections if needed
â€¢ Use **bold** for key concepts
â€¢ Use > for important quotes
â€¢ Timecodes in (MM:SS) format for each claim

ğŸ“š INTERACTIVE CONCEPTS (IMPORTANT):
Wrap technical terms, proper nouns and important concepts with [[double brackets]].
Examples:
â€¢ "[[Photosynthesis]] allows plants to..."
â€¢ "According to [[Albert Einstein]], [[general relativity]] explains..."
â€¢ "[[GDP]] increased by 3%..."
â€¢ "This technique uses [[deep learning]]..."
Mark 3-6 concepts per response, NO MORE. Choose the most important/educational ones.

â±ï¸ CITATIONS WITH TIMECODES:
âœ… "According to the author at **(5:23)**, concept X means..."
âœ… "At **(12:45)**, it is explained that..."

âš ï¸ IMPORTANT - VIDEO vs REALITY:
â€¢ If question is about VERIFIABLE FACTS (dates, events):
  â†’ Specify "**In this video**, it is said that..."
â€¢ If it's a HUMOROUS/PARODY video:
  â†’ Mention it: "This video is a **parody**..."

ğŸš« PROHIBITIONS:
â€¢ No preambles ("Great question", "Certainly")
â€¢ No generic conclusions
â€¢ Don't hallucinate information not present
â€¢ NEVER mix languages

âœ… EXPECTED BEHAVIOR:
â€¢ DIRECT answer from the first sentence
â€¢ Clear professional structuring
â€¢ Honesty if info isn't in the video

ğŸ’¡ REFLECTION QUESTIONS (MANDATORY):
At the end of EACH response, add a section with EXACTLY this format:

---
**ğŸ”® To go further:**
[ask:Relevant question 1 based on video content]
[ask:Relevant question 2 to deepen understanding]
[ask:Relevant question 3 to broaden reflection]

âš ï¸ CRITICAL FORMAT: Each question MUST be on its own line, start with [ask: and end with ]
These questions must be concrete, specific to the discussed topic, and encourage the user to explore further.
"""
        
        user_prompt = f"""ğŸ“‹ VIDEO SUMMARY:
{summary[:4000] if summary else "Not available"}

ğŸ“ FULL TRANSCRIPT:
{transcript_truncated}

ğŸ’¬ CHAT HISTORY:{history_text}

â“ USER QUESTION: {question}

ğŸ“ Respond in an EXPERT and STRUCTURED manner:"""

    return system_prompt, user_prompt


async def generate_chat_response(
    question: str,
    video_title: str,
    transcript: str,
    summary: str,
    chat_history: List[Dict],
    mode: str = "standard",
    lang: str = "fr",
    model: str = "mistral-small-latest",
    api_key: str = None
) -> Optional[str]:
    """GÃ©nÃ¨re une rÃ©ponse de chat intelligente et adaptÃ©e avec Mistral"""
    api_key = api_key or get_mistral_key()
    if not api_key:
        return None
    
    system_prompt, user_prompt = build_chat_prompt(
        question, video_title, transcript, summary, chat_history, mode, lang
    )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ§  TOKENS ADAPTATIFS selon le type de question
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    question_lower = question.lower()
    word_count = len(question.split())
    
    # Questions simples = rÃ©ponses adaptÃ©es mais pas trop courtes
    is_simple = word_count < 8 or any(p in question_lower for p in [
        "est-ce que", "c'est quoi", "qui est", "is it", "what is", "who is"
    ])
    
    base_tokens = {
        "accessible": 2500,
        "standard": 4000,
        "expert": 6000
    }.get(mode, 4000)
    
    # RÃ©ponses plus courtes pour questions simples, mais suffisamment longues
    max_tokens = min(base_tokens, 2500) if is_simple else base_tokens
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.mistral.ai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "max_tokens": max_tokens,
                    "temperature": 0.2  # RÃ©duit pour plus de prÃ©cision
                },
                timeout=60
            )
            
            if response.status_code == 200:
                answer = response.json()["choices"][0]["message"]["content"].strip()
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ğŸ§¹ POST-PROCESSING: Supprimer les prÃ©ambules indÃ©sirables
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                preambles_to_remove = [
                    "Bien sÃ»r!", "Bien sÃ»r,", "Bien sÃ»r ", "Certainement!", "Certainement,",
                    "Excellente question!", "Bonne question!", "C'est une bonne question.",
                    "C'est une excellente question.", "Je vais rÃ©pondre Ã  votre question.",
                    "Permettez-moi de rÃ©pondre.", "Avec plaisir!", "Avec plaisir,",
                    "Sure!", "Certainly!", "Great question!", "Good question!",
                    "Let me answer that.", "I'll explain.", "Of course!",
                    "That's a great question.", "Happy to help!"
                ]
                
                for preamble in preambles_to_remove:
                    if answer.startswith(preamble):
                        answer = answer[len(preamble):].strip()
                        break
                
                # Supprimer aussi les formules de politesse de fin
                endings_to_trim = [
                    "\n\nN'hÃ©sitez pas Ã  poser d'autres questions!",
                    "\n\nN'hÃ©sitez pas si vous avez d'autres questions.",
                    "\n\nJ'espÃ¨re que cela rÃ©pond Ã  votre question.",
                    "\n\nJ'espÃ¨re que cela vous aide!",
                    "\n\nFeel free to ask more questions!",
                    "\n\nHope this helps!",
                    "\n\nLet me know if you have more questions."
                ]
                
                for ending in endings_to_trim:
                    if answer.endswith(ending):
                        answer = answer[:-len(ending)].strip()
                        break
                
                return answer
            else:
                print(f"âŒ Chat API error: {response.status_code}", flush=True)
                return None
                
    except Exception as e:
        print(f"âŒ Chat generation error: {e}", flush=True)
        return None


async def generate_chat_response_stream(
    question: str,
    video_title: str,
    transcript: str,
    summary: str,
    chat_history: List[Dict],
    mode: str = "standard",
    lang: str = "fr",
    model: str = "mistral-small-latest",
    api_key: str = None
) -> AsyncGenerator[str, None]:
    """GÃ©nÃ¨re une rÃ©ponse de chat en streaming"""
    api_key = api_key or get_mistral_key()
    if not api_key:
        yield "Error: API key not configured"
        return
    
    system_prompt, user_prompt = build_chat_prompt(
        question, video_title, transcript, summary, chat_history, mode, lang
    )
    
    max_tokens = {"accessible": 2500, "standard": 4000, "expert": 6000}.get(mode, 4000)
    
    try:
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                "https://api.mistral.ai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "max_tokens": max_tokens,
                    "temperature": 0.3,
                    "stream": True
                },
                timeout=120
            ) as response:
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            content = chunk["choices"][0]["delta"].get("content", "")
                            if content:
                                yield content
                        except:
                            continue
    except Exception as e:
        yield f"Error: {e}"


async def generate_chat_response_v4(
    question: str,
    video_title: str,
    transcript: str,
    summary: str,
    chat_history: List[Dict],
    user_plan: str,
    mode: str = "standard",
    lang: str = "fr",
    model: str = "mistral-small-latest",
    web_search_requested: bool = False
) -> Tuple[str, List[Dict[str, str]], bool]:
    """
    ğŸ†• v5.0: GÃ©nÃ¨re une rÃ©ponse chat avec FACT-CHECKING INTELLIGENT.
    
    NOUVEAUTÃ‰ v5.0:
    - DÃ©tection automatique des questions factuelles critiques
    - Fact-checking Perplexity mÃªme pour Starter (quota limitÃ©: 10/jour)
    - Avertissement automatique si les faits ne peuvent pas Ãªtre vÃ©rifiÃ©s
    
    Args:
        question: Question de l'utilisateur
        video_title: Titre de la vidÃ©o
        transcript: Transcription
        summary: RÃ©sumÃ© de la vidÃ©o
        chat_history: Historique du chat
        user_plan: Plan de l'utilisateur
        mode: Mode d'analyse
        lang: Langue
        model: ModÃ¨le Mistral
        web_search_requested: Si l'utilisateur a demandÃ© explicitement une recherche web
    
    Returns:
        Tuple[response, sources, web_search_used]
    """
    print(f"ğŸ’¬ [CHAT v5.0] Generating response for plan: {user_plan}", flush=True)
    
    # ğŸ†• v5.0: DÃ©tecter si la question nÃ©cessite un fact-checking critique
    needs_fact_check = _needs_critical_fact_check(question)
    if needs_fact_check:
        print(f"âš ï¸ [CHAT v5.0] Critical fact-check needed for question", flush=True)
    
    # 1. GÃ©nÃ©rer la rÃ©ponse de base avec Mistral
    base_response = await generate_chat_response(
        question=question,
        video_title=video_title,
        transcript=transcript,
        summary=summary,
        chat_history=chat_history,
        mode=mode,
        lang=lang,
        model=model
    )
    
    if not base_response:
        return "DÃ©solÃ©, je n'ai pas pu gÃ©nÃ©rer de rÃ©ponse.", [], False
    
    print(f"âœ… [CHAT v5.0] Base response: {len(base_response)} chars", flush=True)
    
    # 2. Enrichir avec Perplexity si disponible et autorisÃ©
    sources = []
    web_search_used = False
    fact_checked = False
    
    if ENRICHMENT_AVAILABLE:
        enrichment_level = get_enrichment_level(user_plan)
        
        # DÃ©terminer si on doit enrichir
        should_enrich = False
        
        if web_search_requested:
            # L'utilisateur a demandÃ© explicitement une recherche web
            if enrichment_level != EnrichmentLevel.NONE:
                should_enrich = True
                print(f"ğŸŒ [CHAT v5.0] Web search requested by user", flush=True)
        
        # ğŸ†• v5.0: Fact-checking automatique pour questions critiques
        elif needs_fact_check:
            # Pour les questions factuelles critiques, on ESSAIE de vÃ©rifier
            # mÃªme pour les plans basiques (Starter a un quota limitÃ©)
            if user_plan in ["pro", "expert", "unlimited"]:
                should_enrich = True
                print(f"ğŸ” [CHAT v5.0] Critical fact-check triggered (premium plan)", flush=True)
            elif user_plan == "starter":
                # Starter: fact-checking limitÃ© (via le quota web_search)
                # Le quota sera vÃ©rifiÃ© dans enrich_chat_response
                should_enrich = True
                print(f"ğŸ” [CHAT v5.0] Critical fact-check triggered (starter, limited)", flush=True)
            else:
                # Free: pas de fact-checking, mais on ajoute un avertissement
                print(f"âš ï¸ [CHAT v5.0] Fact-check needed but not available for free plan", flush=True)
        
        # Enrichissement automatique standard pour Pro/Expert
        elif enrichment_level in [EnrichmentLevel.FULL, EnrichmentLevel.DEEP]:
            should_enrich = _should_auto_enrich_chat(question, video_title)
            if should_enrich:
                print(f"ğŸŒ [CHAT v5.0] Auto-enrichment triggered for {enrichment_level.value}", flush=True)
        
        if should_enrich:
            try:
                video_context = f"VidÃ©o: {video_title}\n\nRÃ©sumÃ©: {summary[:1500]}"
                
                # ğŸ†• v5.0: Pour Starter, on force le niveau FULL temporairement
                effective_plan = user_plan
                if needs_fact_check and user_plan == "starter":
                    effective_plan = "pro"  # Utiliser les paramÃ¨tres Pro pour le fact-check
                
                enriched_response, sources, actual_level = await enrich_chat_response(
                    question=question,
                    base_response=base_response,
                    video_context=video_context,
                    plan=effective_plan,
                    lang=lang
                )
                
                if sources:
                    base_response = enriched_response
                    web_search_used = True
                    fact_checked = True
                    print(f"âœ… [CHAT v5.0] Enriched with {len(sources)} sources", flush=True)
                    
            except Exception as e:
                print(f"âš ï¸ [CHAT v5.0] Enrichment failed: {e}", flush=True)
    
    # ğŸ†• v5.0: Ajouter un avertissement si fact-check nÃ©cessaire mais non effectuÃ©
    if needs_fact_check and not fact_checked:
        disclaimer = _get_fact_check_disclaimer(lang, user_plan)
        if disclaimer:
            base_response = f"{base_response}\n\n{disclaimer}"
            print(f"âš ï¸ [CHAT v5.0] Added fact-check disclaimer", flush=True)
    
    return base_response, sources, web_search_used


def _get_fact_check_disclaimer(lang: str, plan: str) -> str:
    """
    ğŸ†• v5.0: GÃ©nÃ¨re un avertissement pour les rÃ©ponses non vÃ©rifiÃ©es.
    """
    if lang == "fr":
        if plan == "free":
            return "âš ï¸ **Note**: Cette rÃ©ponse est basÃ©e uniquement sur le contenu de la vidÃ©o. Les dates et faits mentionnÃ©s peuvent Ãªtre inexacts ou fictifs (ex: parodie). Passez au plan Starter pour activer la vÃ©rification des faits."
        else:
            return "âš ï¸ **Note**: Cette rÃ©ponse est basÃ©e sur le contenu de la vidÃ©o. Pour une vÃ©rification des faits actualisÃ©e, activez la recherche web avec le bouton ğŸ”."
    else:
        if plan == "free":
            return "âš ï¸ **Note**: This answer is based only on the video content. Dates and facts mentioned may be inaccurate or fictional (e.g., parody). Upgrade to Starter for fact-checking."
        else:
            return "âš ï¸ **Note**: This answer is based on the video content. For up-to-date fact-checking, enable web search with the ğŸ” button."


def _should_auto_enrich_chat(question: str, video_title: str) -> bool:
    """
    DÃ©termine si une question devrait automatiquement dÃ©clencher 
    un enrichissement Perplexity (pour Pro/Expert).
    """
    question_lower = question.lower()
    
    # Mots-clÃ©s qui dÃ©clenchent l'enrichissement
    TRIGGER_KEYWORDS = [
        # VÃ©rification
        "vrai", "faux", "vÃ©rifier", "confirmer", "exact", "correct",
        "true", "false", "verify", "confirm", "accurate",
        # ActualitÃ©
        "actuel", "rÃ©cent", "aujourd'hui", "maintenant", "derniÃ¨re",
        "current", "recent", "today", "now", "latest",
        # Sources
        "source", "preuve", "Ã©tude", "recherche", "donnÃ©es",
        "evidence", "study", "research", "data",
        # Comparaison
        "comparer", "diffÃ©rence", "alternative", "autre",
        "compare", "difference", "alternative", "other",
        # Questions factuelles
        "combien", "quand", "oÃ¹", "qui a", "statistique",
        "how many", "when", "where", "who", "statistic"
    ]
    
    # VÃ©rifier si la question contient des mots-clÃ©s dÃ©clencheurs
    for keyword in TRIGGER_KEYWORDS:
        if keyword in question_lower:
            return True
    
    # Questions longues et complexes mÃ©ritent souvent un enrichissement
    if len(question.split()) > 15:
        return True
    
    return False


def _needs_critical_fact_check(question: str) -> bool:
    """
    ğŸ†• v5.0: DÃ©tecte les questions qui NÃ‰CESSITENT une vÃ©rification factuelle.
    Ces questions concernent des faits vÃ©rifiables qui peuvent Ãªtre FAUX dans la vidÃ©o.
    
    Exemples:
    - "Quand est sorti de prison X ?" â†’ Date vÃ©rifiable
    - "Qui est le prÃ©sident actuel ?" â†’ Fait actuel
    - "Quel est le prix de X aujourd'hui ?" â†’ DonnÃ©e dynamique
    """
    question_lower = question.lower()
    
    # 1. Questions sur des DATES spÃ©cifiques
    DATE_PATTERNS = [
        "quand", "quelle date", "Ã  quelle date", "depuis quand",
        "when", "what date", "since when",
        "en quelle annÃ©e", "quel jour", "quel mois",
        "date de", "jour de", "annÃ©e de"
    ]
    
    # 2. Questions sur des Ã‰VÃ‰NEMENTS RÃ‰CENTS (2024-2025)
    RECENT_EVENT_PATTERNS = [
        "rÃ©cemment", "derniÃ¨rement", "actuellement", "en ce moment",
        "recently", "currently", "right now", "at the moment",
        "2024", "2025", "cette annÃ©e", "ce mois", "cette semaine",
        "sorti de prison", "Ã©lu", "nommÃ©", "dÃ©cÃ©dÃ©", "mort",
        "dÃ©missionnÃ©", "arrÃªtÃ©", "condamnÃ©", "libÃ©rÃ©"
    ]
    
    # 3. Questions sur des PERSONNES PUBLIQUES + faits vÃ©rifiables
    PERSON_FACT_PATTERNS = [
        "est-il", "est-elle", "a-t-il", "a-t-elle",
        "is he", "is she", "did he", "did she", "has he", "has she",
        "oÃ¹ est", "oÃ¹ habite", "que fait", "que devient",
        "where is", "what happened to"
    ]
    
    # 4. Questions sur des DONNÃ‰ES qui changent
    DYNAMIC_DATA_PATTERNS = [
        "quel est le prix", "combien coÃ»te", "quel est le score",
        "what is the price", "how much", "what is the score",
        "population", "taux", "pourcentage actuel",
        "classement", "ranking", "position"
    ]
    
    # VÃ©rifier chaque catÃ©gorie
    all_patterns = DATE_PATTERNS + RECENT_EVENT_PATTERNS + PERSON_FACT_PATTERNS + DYNAMIC_DATA_PATTERNS
    
    for pattern in all_patterns:
        if pattern in question_lower:
            return True
    
    # DÃ©tection de questions sur des personnes + Ã©vÃ©nements
    # Ex: "Sarkozy prison" ou "Macron dÃ©mission"
    FAMOUS_NAMES = [
        "sarkozy", "macron", "trump", "biden", "poutine", "putin",
        "musk", "zuckerberg", "bezos", "gates",
        "mbappÃ©", "mbappe", "messi", "ronaldo"
    ]
    
    for name in FAMOUS_NAMES:
        if name in question_lower:
            # Si c'est une question sur une personne cÃ©lÃ¨bre, vÃ©rifier
            if any(word in question_lower for word in ["quand", "when", "oÃ¹", "where", "fait", "did"]):
                return True
    
    return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” PERPLEXITY (Recherche Web) - LEGACY + v4.0
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def check_web_search_quota(
    session: AsyncSession,
    user_id: int
) -> Tuple[bool, int, int]:
    """
    VÃ©rifie le quota de recherche web.
    Retourne: (can_search, used, limit)
    """
    result = await session.execute(select(User).where(User.id == user_id))
    user = result.scalar_one_or_none()
    
    if not user:
        return False, 0, 0
    
    plan = user.plan or "free"
    limits = PLAN_LIMITS.get(plan, PLAN_LIMITS["free"])
    
    if not limits.get("web_search_enabled", False):
        return False, 0, 0
    
    monthly_limit = limits.get("web_search_monthly", 0)
    if monthly_limit == -1:
        return True, 0, -1  # IllimitÃ©
    
    # VÃ©rifier l'usage ce mois
    month = date.today().strftime("%Y-%m")
    usage_result = await session.execute(
        select(WebSearchUsage).where(
            WebSearchUsage.user_id == user_id,
            WebSearchUsage.month_year == month
        )
    )
    usage = usage_result.scalar_one_or_none()
    used = usage.search_count if usage else 0
    
    return used < monthly_limit, used, monthly_limit


async def increment_web_search_usage(session: AsyncSession, user_id: int):
    """IncrÃ©mente le compteur de recherche web"""
    month = date.today().strftime("%Y-%m")
    
    result = await session.execute(
        select(WebSearchUsage).where(
            WebSearchUsage.user_id == user_id,
            WebSearchUsage.month_year == month
        )
    )
    usage = result.scalar_one_or_none()
    
    if usage:
        usage.search_count += 1
        usage.last_search_at = datetime.now()
    else:
        usage = WebSearchUsage(
            user_id=user_id,
            month_year=month,
            search_count=1,
            last_search_at=datetime.now()
        )
        session.add(usage)
    
    await session.commit()


async def search_with_perplexity(
    question: str,
    context: str,
    lang: str = "fr"
) -> Optional[str]:
    """Fait une recherche web avec Perplexity (Legacy - utilisÃ© pour le chat explicite)"""
    api_key = get_perplexity_key()
    if not api_key:
        return None
    
    prompt = f"""Recherche des informations actuelles sur cette question en lien avec le contexte suivant.

Contexte: {context[:2000]}

Question: {question}

RÃ©ponds en {"franÃ§ais" if lang == "fr" else "anglais"} avec des sources web rÃ©centes."""
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.perplexity.ai/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "llama-3.1-sonar-small-128k-online",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 2500,
                    "temperature": 0.2
                },
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            return None
    except Exception as e:
        print(f"âŒ Perplexity error: {e}", flush=True)
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ FONCTION PRINCIPALE DE CHAT v4.0
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def process_chat_message_v4(
    session: AsyncSession,
    user_id: int,
    summary_id: int,
    question: str,
    web_search: bool = False,
    mode: str = "standard"
) -> Dict[str, Any]:
    """
    ğŸ†• v4.0: Traite un message chat avec enrichissement progressif.
    
    Returns:
        {
            "response": str,
            "web_search_used": bool,
            "sources": List[Dict],
            "enrichment_level": str,
            "quota_info": Dict
        }
    """
    # 1. VÃ©rifier les quotas
    can_ask, reason, quota_info = await check_chat_quota(session, user_id, summary_id)
    if not can_ask:
        return {
            "response": f"âŒ Limite atteinte: {reason}",
            "web_search_used": False,
            "sources": [],
            "enrichment_level": "none",
            "quota_info": quota_info,
            "error": reason
        }
    
    # 2. RÃ©cupÃ©rer le rÃ©sumÃ© et le contexte
    result = await session.execute(select(Summary).where(Summary.id == summary_id))
    summary = result.scalar_one_or_none()
    
    if not summary:
        return {
            "response": "âŒ RÃ©sumÃ© non trouvÃ©",
            "web_search_used": False,
            "sources": [],
            "enrichment_level": "none",
            "quota_info": quota_info,
            "error": "summary_not_found"
        }
    
    # 3. RÃ©cupÃ©rer l'utilisateur pour le plan
    user_result = await session.execute(select(User).where(User.id == user_id))
    user = user_result.scalar_one_or_none()
    user_plan = user.plan if user else "free"
    
    # 4. RÃ©cupÃ©rer l'historique
    chat_history = await get_chat_history(session, summary_id, user_id, limit=10)
    
    # 5. DÃ©terminer le modÃ¨le selon le plan
    plan_limits = PLAN_LIMITS.get(user_plan, PLAN_LIMITS["free"])
    model = plan_limits.get("default_model", "mistral-small-latest")
    
    # 6. GÃ©nÃ©rer la rÃ©ponse avec enrichissement v4.0
    response, sources, web_search_used = await generate_chat_response_v4(
        question=question,
        video_title=summary.video_title,
        transcript=summary.transcript_context or "",
        summary=summary.summary_content,
        chat_history=chat_history,
        user_plan=user_plan,
        mode=mode,
        lang=summary.lang or "fr",
        model=model,
        web_search_requested=web_search
    )
    
    # 7. DÃ©terminer le niveau d'enrichissement AVANT de sauvegarder
    enrichment_level = "none"
    if ENRICHMENT_AVAILABLE:
        level = get_enrichment_level(user_plan)
        enrichment_level = level.value
    
    # 8. Sauvegarder les messages avec mÃ©tadonnÃ©es v5.0
    await save_chat_message(session, user_id, summary_id, "user", question)
    await save_chat_message(
        session, user_id, summary_id, "assistant", response,
        web_search_used=web_search_used,
        fact_checked=web_search_used and len(sources) > 0,
        sources=sources,
        enrichment_level=enrichment_level if ENRICHMENT_AVAILABLE else None
    )
    
    # 9. IncrÃ©menter les quotas
    await increment_chat_quota(session, user_id)
    if web_search_used:
        await increment_web_search_usage(session, user_id)
    
    return {
        "response": response,
        "web_search_used": web_search_used,
        "sources": sources,
        "enrichment_level": enrichment_level,
        "quota_info": quota_info
    }
